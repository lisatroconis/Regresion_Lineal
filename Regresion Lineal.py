# -*- coding: utf-8 -*-
"""Modulo 6  Regresión lineal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pW_9-1dEAGh0Uw-fwcP9wrmgSjBLRTIP

# Modulo 6: SKLEARN Y STATMODELS, REGRESIÓN LINEAL SIMPLE

Importación de librerias
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm

"""# Preparación de la data

Conexión al drive
"""

from google.colab import files
uploaded=files.upload()

"""Importación de la información"""

df_data = pd.read_excel("final_df (1).xlsx")
display(df_data.shape)
display(df_data.head())

"""Visualización de variables"""

df_data.columns

"""Cambiando el nombre de las variables"""

df_data=df_data.rename(columns={'Nivel_Educativo':'Nivel Educativo','tipo_vivienda':'Tipo Vivienda'})

df_data.columns

"""# Modelo de regresión lineal simple

Vamos a estimar un modelo en donde se analice la distribución de la variable Ingreso en función del Nivel educativo

Gráfico de variables
"""

f, axs = plt.subplots(1, 2,figsize=(15, 6))
sns.boxplot(data=df_data["Ingresos"], ax=axs[0])
axs[0].set_title('Ingresos')

sns.boxplot(data=df_data["Nivel Educativo"], ax=axs[1])
axs[1].set_title('Nivel educativo')

"""Eliminación de datos atípicos"""

q1 = df_data.Ingresos.quantile(0.25)
q3 = df_data.Ingresos.quantile(0.75)
RIQ = q3-q1

#f, axs = plt.subplots(1, 2)
sns.boxplot(data=df_data["Ingresos"])
plt.axhline(min(df_data.Ingresos), color='g')
plt.axhline(q3+1.5*RIQ, color='b')
plt.axhline(q3+3*RIQ, color='r')
sns.boxplot(data=df_data["Nivel Educativo"])

"""###  Gráfico dinámico"""

def plot_function(x_mill = 9000000,y=50000, bins = 10, color='red'):
    binwidth = (max(df_data.Ingresos) - min(df_data.Ingresos))/ bins
    plt.hist(df_data.Ingresos, 
             bins=np.arange(min(df_data.Ingresos), max(df_data.Ingresos) + binwidth, binwidth),
             color=color)
    plt.xlim(0,x_mill)
    plt.ylim(0,y)
    plt.show()

from ipywidgets import interact 
interact(plot_function,
         x_mill = (0, 9000000, 100000),
         y = (0, 50000, 1000),
         bins = (1, 1000, 1),
         color=['red', 'yellow', 'blue','gray','green','white','black'])
None

"""### Selección de una muestra para correr el modelo

Inicialmente vamos a correr el modelo con datos que se encuentren dentro del recorrido intercuantil y además vamos a tomar sólo una muestra al azar de 100 de ellos para ver mejor las gráficas.
"""

df_data.head()

from numpy.core.fromnumeric import repeat
indices = df_data[(df_data.Ingresos<q3) & (df_data.Ingresos>q1)].index
#display(indices)
muestra=np.random.choice(indices,100,replace=False)
display(muestra)
df_data=df_data.loc[muestra]

"""#### Eliminación de datos atípicos anormales"""

df_data = df_data[df_data.Ingresos<q3+1.5*RIQ]
df_data.shape

"""## Diagrama de dispersión de Ingresos y Nivel educativo"""

fig, ax = plt.subplots(figsize=(10, 5))

df_data.plot(
    x    = 'Nivel Educativo',
    y    = 'Ingresos',
    c    = 'green',
    kind = "scatter",
    ax   = ax
)
ax.set_title('Ingresos vs NIvel educativo');

fig, ax = plt.subplots(figsize=(10, 5))

df_data.plot(
    x    = 'Bienes',
    y    = 'Ingresos',
    c    = 'green',
    kind = "scatter",
    ax   = ax
)
ax.set_title('Ingresos vs Bienes');

"""## Correlación lineal entre las variables"""

from scipy.stats import pearsonr
corr_test = pearsonr(x = df_data['Nivel Educativo'], y =  df_data['Ingresos'])
print("Coeficiente de correlación de Pearson: ", corr_test[0])
print("P-value: ", corr_test[1])

corr_test #1er valor correlacion, segundo al p value

from scipy.stats import pearsonr
corr_test = pearsonr(x = df_data['Bienes'], y =  df_data['Ingresos'])
print("Coeficiente de correlación de Pearson: ", str(round(corr_test[0],4)*100) + '%')
#print("P-value: ", corr_test[1])

"""## Estimación del modelo

Selección de variables independientes
"""

inde=['Nivel Educativo']
X=df_data[inde]
X.head(3)

"""Agregar la constante al conjunto de variables independientes"""

X = sm.add_constant(X)
X.head()

"""Selección de variable dependiente"""

y=df_data['Ingresos']
y.head(3)

"""Separación de registros de entrenamiento y prueba"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)

display(X_train.head(2))
display(y_train.head(2))
display(X_test.head(2))
display(y_test.head(2))

display(X_train.head(2))
display(X_test.head(2))

"""Estimación del modelo"""

model = sm.OLS(y_train,X_train)
results = model.fit()

"""Visualizacion de resultados"""

print(results.summary())

"""Parámetros del modelo"""

results.params

display(X_test.head(2))
display(y_test.head(2))

"""## Intervalos de confianza"""

results.conf_int(alpha=0.05) #Confianza del 95%

"""# Predicciones"""

display(X_test.head(2))

display(y_test.head(2))

results.predict(X_test).head(2)

"""## Intervalos de confianza predicciones"""

Pred_test=results.get_prediction(X_test).summary_frame().sort_values("mean")

Pred_test.head(5)

"""Agregando variables independientes a las predicciones"""

Pred_test["X"]=X_test['Nivel Educativo']
Pred_test["Observado"]= y_test

Pred_test.head(5)

Pred_test=Pred_test.sort_values("mean")

Pred_test.head(5)

"""## Principales gráficos de la regresión lineal"""

fig, ax = plt.subplots(figsize=(10, 10))

ax.scatter(Pred_test["X"], Pred_test.Observado, marker='o', color = "blue")
ax.plot(Pred_test["X"], Pred_test["mean"], linestyle='-', label="OLS",color="red")
ax.plot(Pred_test["X"], Pred_test["mean_ci_lower"], linestyle='--', color='red', label="95% CI media")
ax.plot(Pred_test["X"], Pred_test["mean_ci_upper"], linestyle='--', color='red')
ax.fill_between(Pred_test["X"], Pred_test["mean_ci_lower"], Pred_test["mean_ci_upper"], alpha=0.1)
ax.plot(Pred_test["X"], Pred_test["obs_ci_lower"], linestyle='--', color='blue', label="95% CI Observaciones")
ax.plot(Pred_test["X"], Pred_test["obs_ci_upper"], linestyle='--', color='blue')


ax.legend();

"""## Pruebas de bondad de ajuste"""

print('coefficient of determination:', results.rsquared)
print('adjusted coefficient of determination:', results.rsquared_adj)
print('regression coefficients:', results.params)

"""## Diagnóstico de los residuos"""

residuos_test   = Pred_test["mean"] - Pred_test["Observado"]

fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(18, 16))

axes[0, 0].scatter(Pred_test["Observado"],Pred_test["mean"], edgecolors=(0, 0, 0), alpha = 0.4)
axes[0, 0].plot([Pred_test["Observado"].min(), Pred_test["Observado"].max()],
                [Pred_test["Observado"].min(), Pred_test["Observado"].max()],
                'k--', color = 'black', lw=2)
axes[0, 0].set_title('Valor predicho vs valor real', fontsize = 10, fontweight = "bold")
axes[0, 0].set_xlabel('Real')
axes[0, 0].set_ylabel('Predicción')
axes[0, 0].tick_params(labelsize = 7)

axes[0, 1].scatter(list(range(len(residuos_test))), residuos_test,
                   edgecolors=(0, 0, 0), alpha = 0.4)
axes[0, 1].axhline(y = 0, linestyle = '--', color = 'black', lw=2)
axes[0, 1].set_title('Residuos del modelo', fontsize = 10, fontweight = "bold")
axes[0, 1].set_xlabel('id')
axes[0, 1].set_ylabel('Residuo')
axes[0, 1].tick_params(labelsize = 7)

sns.histplot(
    data    = residuos_test,
    stat    = "density",
    kde     = True,
    line_kws= {'linewidth': 1},
    color   = "firebrick",
    alpha   = 0.3,
    ax      = axes[1, 0]
)

axes[1, 0].set_title('Distribución residuos del modelo', fontsize = 10,
                     fontweight = "bold")
axes[1, 0].set_xlabel("Residuo")
axes[1, 0].tick_params(labelsize = 7)


sm.qqplot(
    residuos_test,
    fit   = True,
    line  = 'q',
    ax    = axes[1, 1], 
    color = 'firebrick',
    alpha = 0.4,
    lw    = 2
)
axes[1, 1].set_title('Q-Q residuos del modelo', fontsize = 10, fontweight = "bold")
axes[1, 1].tick_params(labelsize = 7)

axes[2, 0].scatter(Pred_test["mean"], residuos_test,
                   edgecolors=(0, 0, 0), alpha = 0.4)
axes[2, 0].axhline(y = 0, linestyle = '--', color = 'black', lw=2)
axes[2, 0].set_title('Residuos del modelo vs predicción', fontsize = 10, fontweight = "bold")
axes[2, 0].set_xlabel('Predicción')
axes[2, 0].set_ylabel('Residuo')
axes[2, 0].tick_params(labelsize = 7)

# Se eliminan los axes vacíos
fig.delaxes(axes[2,1])

fig.tight_layout()
plt.subplots_adjust(top=0.9)
fig.suptitle('Diagnóstico residuos', fontsize = 12, fontweight = "bold");

"""## Test de Normalidad

Shapiro test
"""

from scipy import stats
shapiro_test = stats.shapiro(residuos_test)
shapiro_test

k2, p_value = stats.normaltest(residuos_test)
print('Estadístico: ' + str(k2)+'p_value: ' + str(p_value))